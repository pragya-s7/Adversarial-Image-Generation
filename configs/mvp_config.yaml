# MVP Configuration for Grounded Attention
# Minimal configuration for quick proof of concept

# Model Configuration
model:
  name: "llava-hf/llava-1.5-7b-hf"
  grounded_layer_indices: [28, 29, 30, 31]  # Last 4 layers
  grounding_type: "similarity"  # Options: similarity, attention_weighted, learnable
  grounding_strength: 1.0
  use_grounding: true
  freeze_vision_encoder: true
  load_in_8bit: false
  load_in_4bit: false

# Data Configuration
data:
  train_data_root: "data/coco/train2014"
  val_data_root: "data/coco/val2014"
  train_annotation_file: "data/coco/annotations/captions_train2014.json"
  val_annotation_file: "data/coco/annotations/captions_val2014.json"
  max_length: 512
  num_workers: 4
  include_negatives: false  # Set to true when hallucination data is available
  negative_ratio: 0.3

# Training Configuration
training:
  batch_size: 4
  num_epochs: 3
  learning_rate: 2e-5
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.999
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  gradient_accumulation_steps: 1
  mixed_precision: "fp16"  # Options: no, fp16, bf16

  # Loss weights
  lambda_grounding: 0.5
  lambda_contrastive: 0.1
  grounding_loss_type: "margin"  # Options: margin, bce, mse
  contrastive_margin: 0.5

# Evaluation Configuration
evaluation:
  eval_batch_size: 8
  max_new_tokens: 100
  temperature: 0.7
  do_sample: true

  # Benchmarks to run
  run_pope: false  # Set to true when POPE dataset is available
  run_chair: false  # Set to true when COCO dataset is available
  run_mme: false  # Set to true when MME dataset is available

# Logging Configuration
logging:
  output_dir: "outputs/mvp"
  log_interval: 10
  eval_interval: 500
  save_interval: 1000
  wandb_project: "grounded-attention-mvp"
  experiment_name: "mvp-similarity-last4"
  log_grounding_scores: true

# Hardware Configuration
hardware:
  device: "cuda"  # cuda or cpu
  num_gpus: 1
  use_distributed: false
